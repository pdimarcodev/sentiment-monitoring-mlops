{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# ðŸ¢ MLOps Sentiment Analysis - Company Reputation Monitoring\n\n**Project**: Online Reputation Monitoring System for MachineInnovators Inc.  \n**GitHub Repository**: https://github.com/pdimarcodev/sentiment-monitoring-mlops  \n**ðŸš€ Live Demo**: https://huggingface.co/spaces/pdimarcodev/sentiment-monitoring-mlops\n\nThis notebook demonstrates the complete MLOps pipeline for sentiment analysis including:\n- âœ… HuggingFace RoBERTa model integration\n- âœ… Model evaluation on public dataset (Tweet Eval)\n- âœ… Performance metrics: accuracy, precision, recall, F1-score\n- âœ… FastAPI service with comprehensive endpoints\n- âœ… Automated testing and CI/CD pipeline\n- âœ… Grafana monitoring and metrics\n- âœ… Docker containerization\n- âœ… Automated model retraining with Airflow\n- âœ… **Live deployment on HuggingFace Spaces**",
   "metadata": {
    "id": "title"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸš€ Setup and Installation"
   ],
   "metadata": {
    "id": "setup"
   }
  },
  {
   "cell_type": "code",
   "source": "# Install required packages\n!pip install transformers torch fastapi uvicorn pydantic requests pandas numpy scikit-learn\n!pip install pytest pytest-asyncio httpx\n!pip install prometheus-client datasets matplotlib seaborn",
   "metadata": {
    "id": "install"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ“¥ Clone Repository and Setup"
   ],
   "metadata": {
    "id": "clone"
   }
  },
  {
   "cell_type": "code",
   "source": "# Clone the repository\n!git clone https://github.com/pdimarcodev/sentiment-monitoring-mlops.git\n%cd sentiment-monitoring-mlops\n\n# List project structure\n!find . -type f -name \"*.py\" | head -20",
   "metadata": {
    "id": "clone_repo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ¤– Load and Test Sentiment Analysis Model"
   ],
   "metadata": {
    "id": "model_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Import our custom sentiment analyzer\n",
    "import sys\n",
    "sys.path.append('/content/sentiment-monitoring-mlops')\n",
    "\n",
    "from src.sentiment_analyzer.model import SentimentAnalyzer\n",
    "import json\n",
    "\n",
    "# Initialize the model\n",
    "print(\"Loading sentiment analysis model...\")\n",
    "analyzer = SentimentAnalyzer()\n",
    "print(\"âœ… Model loaded successfully!\")\n",
    "\n",
    "# Display model info\n",
    "model_info = analyzer.get_model_info()\n",
    "print(f\"\\nðŸ“‹ Model Information:\")\n",
    "for key, value in model_info.items():\n",
    "    print(f\"  {key}: {value}\")"
   ],
   "metadata": {
    "id": "load_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ§ª Test Single Predictions"
   ],
   "metadata": {
    "id": "single_test"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Test with various sentiment examples\n",
    "test_texts = [\n",
    "    \"I absolutely love this company's products! Best service ever!\",\n",
    "    \"This service is terrible and I'm very disappointed.\",\n",
    "    \"The product is okay, nothing particularly special.\",\n",
    "    \"Amazing customer support and fast delivery! Highly recommend!\",\n",
    "    \"Poor quality for the price. Won't buy again.\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ§ª Testing Single Predictions:\\n\")\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    result = analyzer.predict(text)\n",
    "    \n",
    "    # Add emoji based on sentiment\n",
    "    emoji = {\"positive\": \"ðŸ˜Š\", \"negative\": \"ðŸ˜ž\", \"neutral\": \"ðŸ˜\"}\n",
    "    sentiment_emoji = emoji.get(result['sentiment'], '')\n",
    "    \n",
    "    print(f\"{i}. Text: \\\"{text}\\\"\")\n",
    "    print(f\"   {sentiment_emoji} Sentiment: {result['sentiment'].upper()} ({result['confidence']:.2%})\")\n",
    "    print(f\"   All scores: {result['all_scores']}\")\n",
    "    print()"
   ],
   "metadata": {
    "id": "single_predictions"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ“Š Test Batch Predictions"
   ],
   "metadata": {
    "id": "batch_test"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Test batch predictions\n",
    "batch_texts = [\n",
    "    \"Great product, fast shipping!\",\n",
    "    \"Customer service was unhelpful.\",\n",
    "    \"Average quality for the price.\",\n",
    "    \"Exceeded my expectations!\",\n",
    "    \"Worst purchase I've made.\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ“Š Testing Batch Predictions:\\n\")\n",
    "batch_results = analyzer.predict_batch(batch_texts)\n",
    "\n",
    "# Display results\n",
    "sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
    "\n",
    "for i, result in enumerate(batch_results, 1):\n",
    "    emoji = {\"positive\": \"ðŸ˜Š\", \"negative\": \"ðŸ˜ž\", \"neutral\": \"ðŸ˜\"}\n",
    "    sentiment_emoji = emoji.get(result['sentiment'], '')\n",
    "    \n",
    "    print(f\"{i}. {sentiment_emoji} {result['sentiment'].upper()} ({result['confidence']:.2%}): \\\"{result['text']}\\\"\")\n",
    "    sentiment_counts[result['sentiment']] += 1\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Summary:\")\n",
    "total = len(batch_results)\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    percentage = count / total * 100\n",
    "    emoji = {\"positive\": \"ðŸ˜Š\", \"negative\": \"ðŸ˜ž\", \"neutral\": \"ðŸ˜\"}\n",
    "    print(f\"  {emoji[sentiment]} {sentiment.title()}: {count}/{total} ({percentage:.1f}%)\")"
   ],
   "metadata": {
    "id": "batch_predictions"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Show some example predictions\nprint(\"\\nðŸ“ SAMPLE PREDICTIONS\\n\")\nprint(\"=\"*80)\n\n# Show 5 correct predictions\ncorrect_indices = [i for i in range(len(predictions)) if predictions[i] == true_labels[i]]\nprint(\"\\nâœ… Correct Predictions (5 examples):\")\nfor i in correct_indices[:5]:\n    emoji = {\"positive\": \"ðŸ˜Š\", \"negative\": \"ðŸ˜ž\", \"neutral\": \"ðŸ˜\"}\n    sentiment_emoji = emoji.get(predictions[i], '')\n    print(f\"\\n{i+1}. Text: \\\"{test_data[i]['text'][:80]}...\\\"\")\n    print(f\"   True: {true_labels[i]} | Predicted: {predictions[i]} {sentiment_emoji}\")\n\n# Show 5 incorrect predictions\nincorrect_indices = [i for i in range(len(predictions)) if predictions[i] != true_labels[i]]\nif incorrect_indices:\n    print(f\"\\n\\nâŒ Incorrect Predictions (5 examples):\")\n    for i in incorrect_indices[:5]:\n        print(f\"\\n{i+1}. Text: \\\"{test_data[i]['text'][:80]}...\\\"\")\n        print(f\"   True: {true_labels[i]} | Predicted: {predictions[i]} âš ï¸\")\n\nprint(f\"\\n\\nðŸ“Š Evaluation Summary:\")\nprint(f\"   Total samples: {len(predictions)}\")\nprint(f\"   Correct: {len(correct_indices)} ({len(correct_indices)/len(predictions):.1%})\")\nprint(f\"   Incorrect: {len(incorrect_indices)} ({len(incorrect_indices)/len(predictions):.1%})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display confusion matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nprint(\"\\nðŸŽ¯ CONFUSION MATRIX\\n\")\n\n# Calculate confusion matrix\ncm = confusion_matrix(true_labels, predictions, labels=['negative', 'neutral', 'positive'])\n\n# Create visualization\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Negative', 'Neutral', 'Positive'],\n            yticklabels=['Negative', 'Neutral', 'Positive'],\n            cbar_kws={'label': 'Count'})\nplt.title('Confusion Matrix - Sentiment Analysis', fontsize=14, fontweight='bold')\nplt.ylabel('True Label', fontsize=12)\nplt.xlabel('Predicted Label', fontsize=12)\nplt.tight_layout()\nplt.show()\n\n# Print text version\nprint(\"\\nConfusion Matrix (rows=true, cols=predicted):\")\nprint(f\"{'':>12} {'Negative':>12} {'Neutral':>12} {'Positive':>12}\")\nprint(\"-\" * 50)\nfor i, true_label in enumerate(['Negative', 'Neutral', 'Positive']):\n    print(f\"{true_label:>12} {cm[i][0]:>12} {cm[i][1]:>12} {cm[i][2]:>12}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Calculate performance metrics\nprint(\"ðŸ“Š MODEL PERFORMANCE METRICS\\n\")\nprint(\"=\"*60)\n\n# Overall accuracy\naccuracy = accuracy_score(true_labels, predictions)\nprint(f\"\\nâœ… Overall Accuracy: {accuracy:.2%}\")\n\n# Per-class metrics\nprecision, recall, f1, support = precision_recall_fscore_support(\n    true_labels, predictions, labels=['negative', 'neutral', 'positive'], average=None\n)\n\nprint(f\"\\nðŸ“ˆ Per-Class Metrics:\")\nprint(f\"{'Class':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<12}\")\nprint(\"-\" * 60)\nfor i, label in enumerate(['negative', 'neutral', 'positive']):\n    print(f\"{label:<12} {precision[i]:<12.2%} {recall[i]:<12.2%} {f1[i]:<12.2%} {support[i]:<12.0f}\")\n\n# Macro and weighted averages\nmacro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n    true_labels, predictions, average='macro'\n)\nweighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n    true_labels, predictions, average='weighted'\n)\n\nprint(\"\\nðŸ“Š Average Metrics:\")\nprint(f\"  Macro Avg    - Precision: {macro_precision:.2%}, Recall: {macro_recall:.2%}, F1: {macro_f1:.2%}\")\nprint(f\"  Weighted Avg - Precision: {weighted_precision:.2%}, Recall: {weighted_recall:.2%}, F1: {weighted_f1:.2%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Run predictions on test dataset\nprint(\"ðŸ”® Running predictions on test dataset...\")\nprint(\"This may take 2-3 minutes for 500 samples...\\n\")\n\npredictions = []\ntrue_labels = []\n\n# Label mapping for tweet_eval dataset\nlabel_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n\nfor i, sample in enumerate(test_data):\n    if (i + 1) % 100 == 0:\n        print(f\"  Processed {i + 1}/{len(test_data)} samples...\")\n    \n    # Get prediction\n    result = analyzer.predict(sample['text'])\n    predictions.append(result['sentiment'])\n    \n    # Get true label\n    true_labels.append(label_map[sample['label']])\n\nprint(f\"\\nâœ… Completed predictions on {len(predictions)} samples!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Load a public sentiment dataset for evaluation\nfrom datasets import load_dataset\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\nimport numpy as np\n\nprint(\"ðŸ“¥ Loading Twitter Sentiment Dataset...\")\n# Load tweet_eval sentiment dataset (3-class: negative, neutral, positive)\ndataset = load_dataset(\"tweet_eval\", \"sentiment\")\n\n# Use a subset for faster evaluation (first 500 test samples)\ntest_data = dataset['test'].select(range(500))\n\nprint(f\"âœ… Loaded {len(test_data)} test samples\\n\")\nprint(f\"Dataset info:\")\nprint(f\"  - Features: {test_data.features}\")\nprint(f\"  - Sample: {test_data[0]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ðŸ“Š Model Evaluation on Public Dataset\n\nNow let's evaluate the model on a real public dataset to measure its performance.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸš€ Start FastAPI Service"
   ],
   "metadata": {
    "id": "fastapi_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Start the FastAPI service in the background\n",
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "\n",
    "def start_api_server():\n",
    "    \"\"\"Start the FastAPI server in a separate thread\"\"\"\n",
    "    subprocess.run([\"python\", \"main.py\"], cwd=\"/content/sentiment-monitoring-mlops\")\n",
    "\n",
    "# Start the server in background\n",
    "api_thread = threading.Thread(target=start_api_server, daemon=True)\n",
    "api_thread.start()\n",
    "\n",
    "print(\"ðŸš€ Starting FastAPI server...\")\n",
    "time.sleep(10)  # Wait for server to start\n",
    "print(\"âœ… API server should be running on http://localhost:8000\")"
   ],
   "metadata": {
    "id": "start_api"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ§ª Test API Endpoints"
   ],
   "metadata": {
    "id": "api_test_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "\n",
    "# Test health endpoint\n",
    "print(\"ðŸ¥ Testing Health Endpoint:\")\n",
    "try:\n",
    "    response = requests.get(f\"{BASE_URL}/health\", timeout=5)\n",
    "    print(f\"Status: {response.status_code}\")\n",
    "    print(f\"Response: {json.dumps(response.json(), indent=2)}\")\nexcept Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test single prediction endpoint\n",
    "print(\"ðŸ¤– Testing Single Prediction Endpoint:\")\n",
    "try:\n",
    "    payload = {\"text\": \"I love this company's innovative solutions!\"}\n",
    "    response = requests.post(f\"{BASE_URL}/predict\", json=payload, timeout=10)\n",
    "    print(f\"Status: {response.status_code}\")\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        emoji = {\"positive\": \"ðŸ˜Š\", \"negative\": \"ðŸ˜ž\", \"neutral\": \"ðŸ˜\"}\n",
    "        sentiment_emoji = emoji.get(result['sentiment'], '')\n",
    "        print(f\"Text: \\\"{result['text']}\\\"\")\n",
    "        print(f\"Sentiment: {sentiment_emoji} {result['sentiment'].upper()} ({result['confidence']:.2%})\")\n",
    "        print(f\"All scores: {result['all_scores']}\")\n",
    "    else:\n",
    "        print(f\"Error: {response.text}\")\nexcept Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ],
   "metadata": {
    "id": "test_api"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ“Š Test Batch API Endpoint"
   ],
   "metadata": {
    "id": "batch_api_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Test batch prediction endpoint\n",
    "print(\"ðŸ“Š Testing Batch Prediction Endpoint:\")\n",
    "try:\n",
    "    batch_payload = {\n",
    "        \"texts\": [\n",
    "            \"Excellent product quality and service!\",\n",
    "            \"Poor customer experience, very disappointed.\",\n",
    "            \"The product is decent, meets basic needs.\",\n",
    "            \"Outstanding innovation and user experience!\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(f\"{BASE_URL}/predict/batch\", json=batch_payload, timeout=15)\n",
    "    print(f\"Status: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"Total processed: {result['total_processed']}\\n\")\n",
    "        \n",
    "        # Display results\n",
    "        sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
    "        \n",
    "        for i, pred in enumerate(result['results'], 1):\n",
    "            emoji = {\"positive\": \"ðŸ˜Š\", \"negative\": \"ðŸ˜ž\", \"neutral\": \"ðŸ˜\"}\n",
    "            sentiment_emoji = emoji.get(pred['sentiment'], '')\n",
    "            print(f\"{i}. {sentiment_emoji} {pred['sentiment'].upper()} ({pred['confidence']:.2%})\")\n",
    "            print(f\"   Text: \\\"{pred['text']}\\\"\")\n",
    "            sentiment_counts[pred['sentiment']] += 1\n",
    "        \n",
    "        print(f\"\\nðŸ“ˆ Batch Summary:\")\n",
    "        for sentiment, count in sentiment_counts.items():\n",
    "            percentage = count / result['total_processed'] * 100\n",
    "            emoji = {\"positive\": \"ðŸ˜Š\", \"negative\": \"ðŸ˜ž\", \"neutral\": \"ðŸ˜\"}\n",
    "            print(f\"  {emoji[sentiment]} {sentiment.title()}: {count} ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"Error: {response.text}\")\nexcept Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ],
   "metadata": {
    "id": "batch_api_test"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ“ˆ Test Metrics Endpoint"
   ],
   "metadata": {
    "id": "metrics_section"
   }
  },
  {
   "cell_type": "code",
   "source": "# Display comprehensive project summary\nprint(\"ðŸŽ¯ MLOps SENTIMENT ANALYSIS PROJECT SUMMARY\\n\")\nprint(\"=\"*60)\n\nsummary = {\n    \"âœ… Model Implementation\": \"HuggingFace RoBERTa (cardiffnlp/twitter-roberta-base-sentiment-latest)\",\n    \"âœ… Dataset Evaluation\": \"Tweet Eval public dataset with accuracy, precision, recall, F1 metrics\",\n    \"âœ… API Service\": \"FastAPI with /predict, /predict/batch, /health, /metrics endpoints\",\n    \"âœ… Testing Suite\": \"Comprehensive unit and integration tests with pytest\",\n    \"âœ… CI/CD Pipeline\": \"GitHub Actions with automated testing and deployment\",\n    \"âœ… Containerization\": \"Multi-stage Docker build with security best practices\",\n    \"âœ… Monitoring\": \"Grafana + Prometheus with custom dashboards\",\n    \"âœ… Orchestration\": \"Docker Compose for local development\",\n    \"âœ… Model Retraining\": \"Airflow DAG for automated model updates\",\n    \"âœ… HF Spaces Deployment\": \"Live Gradio interface on HuggingFace Spaces\"\n}\n\nfor feature, description in summary.items():\n    print(f\"{feature}\")\n    print(f\"   {description}\\n\")\n\nprint(\"ðŸ”— Key URLs:\")\nprint(\"   â€¢ GitHub Repository: https://github.com/pdimarcodev/sentiment-monitoring-mlops\")\nprint(\"   â€¢ ðŸš€ Live Demo (HF Spaces): https://huggingface.co/spaces/pdimarcodev/sentiment-monitoring-mlops\")\nprint(\"   â€¢ CI/CD Pipeline: https://github.com/pdimarcodev/sentiment-monitoring-mlops/actions\")\nprint(\"   â€¢ Grafana Dashboard: http://localhost:3000 (when running locally)\")\n\nprint(\"\\nðŸš€ Production Deployment Steps:\")\nprint(\"   1. Push code to GitHub repository\")\nprint(\"   2. Configure GitHub Secrets (DOCKER_USERNAME, DOCKER_PASSWORD, HF_TOKEN)\")\nprint(\"   3. Deploy with: docker-compose up -d\")\nprint(\"   4. Access Grafana at http://localhost:3000 (admin/admin123)\")\nprint(\"   5. Monitor metrics and sentiment trends\")\n\nprint(\"\\nâœ¨ MLOps Features Demonstrated:\")\nfeatures = [\n    \"Automated model loading and inference\",\n    \"Model evaluation on public dataset (Tweet Eval)\",\n    \"Performance metrics: accuracy, precision, recall, F1-score\",\n    \"RESTful API with proper error handling\",\n    \"Prometheus metrics collection\",\n    \"Comprehensive testing strategy\",\n    \"CI/CD pipeline with security scanning\",\n    \"Container orchestration with monitoring\",\n    \"Model retraining automation\",\n    \"Production-ready deployment\",\n    \"Live web interface on HuggingFace Spaces\"\n]\n\nfor i, feature in enumerate(features, 1):\n    print(f\"   {i}. {feature}\")\n\nprint(f\"\\nðŸŽ‰ Project completed successfully! All MLOps requirements implemented.\")",
   "metadata": {
    "id": "test_metrics"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ§ª Run Automated Tests"
   ],
   "metadata": {
    "id": "testing_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Run the automated test suite\n",
    "print(\"ðŸ§ª Running Automated Test Suite:\\n\")\n",
    "\n",
    "# Run model tests\n",
    "print(\"Testing model functionality...\")\n",
    "!cd /content/sentiment-monitoring-mlops && python -m pytest tests/test_model.py -v\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Run API tests\n",
    "print(\"Testing API functionality...\")\n",
    "!cd /content/sentiment-monitoring-mlops && python -m pytest tests/test_api.py -v"
   ],
   "metadata": {
    "id": "run_tests"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ³ Docker Build Test"
   ],
   "metadata": {
    "id": "docker_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Test Docker build (if Docker is available)\n",
    "print(\"ðŸ³ Testing Docker Build:\\n\")\n",
    "\n",
    "try:\n",
    "    # Check if Docker is available\n",
    "    !docker --version\n",
    "    \n",
    "    # Build the Docker image\n",
    "    print(\"\\nBuilding Docker image...\")\n",
    "    !cd /content/sentiment-monitoring-mlops && docker build -t sentiment-analyzer:colab-test .\n",
    "    \n",
    "    print(\"\\nâœ… Docker build completed successfully!\")\n",
    "    \n",
    "    # Show image info\n",
    "    !docker images sentiment-analyzer:colab-test\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"âš ï¸ Docker not available in this environment: {e}\")\n",
    "    print(\"Docker build would work in a local environment with Docker installed.\")"
   ],
   "metadata": {
    "id": "docker_test"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ“‹ MLOps Pipeline Summary"
   ],
   "metadata": {
    "id": "summary_section"
   }
  },
  {
   "cell_type": "code",
   "source": "# Display comprehensive project summary\nprint(\"ðŸŽ¯ MLOps SENTIMENT ANALYSIS PROJECT SUMMARY\\n\")\nprint(\"=\"*60)\n\nsummary = {\n    \"âœ… Model Implementation\": \"HuggingFace RoBERTa (cardiffnlp/twitter-roberta-base-sentiment-latest)\",\n    \"âœ… API Service\": \"FastAPI with /predict, /predict/batch, /health, /metrics endpoints\",\n    \"âœ… Testing Suite\": \"Comprehensive unit and integration tests with pytest\",\n    \"âœ… CI/CD Pipeline\": \"GitHub Actions with automated testing and deployment\",\n    \"âœ… Containerization\": \"Multi-stage Docker build with security best practices\",\n    \"âœ… Monitoring\": \"Grafana + Prometheus with custom dashboards\",\n    \"âœ… Orchestration\": \"Docker Compose for local development\",\n    \"âœ… Model Retraining\": \"Airflow DAG for automated model updates\",\n    \"âœ… Deployment Ready\": \"HuggingFace Spaces integration with Gradio UI\"\n}\n\nfor feature, description in summary.items():\n    print(f\"{feature}\")\n    print(f\"   {description}\\n\")\n\nprint(\"ðŸ”— Key URLs:\")\nprint(\"   â€¢ GitHub Repository: https://github.com/pdimarcodev/sentiment-monitoring-mlops\")\nprint(\"   â€¢ Grafana Dashboard: http://localhost:3000 (when running locally)\")\n\nprint(\"\\nðŸš€ Production Deployment Steps:\")\nprint(\"   1. Push code to GitHub repository\")\nprint(\"   2. Configure GitHub Secrets (DOCKER_USERNAME, DOCKER_PASSWORD, HF_TOKEN)\")\nprint(\"   3. Deploy with: docker-compose up -d\")\nprint(\"   4. Access Grafana at http://localhost:3000 (admin/admin123)\")\nprint(\"   5. Monitor metrics and sentiment trends\")\n\nprint(\"\\nâœ¨ MLOps Features Demonstrated:\")\nfeatures = [\n    \"Automated model loading and inference\",\n    \"RESTful API with proper error handling\",\n    \"Prometheus metrics collection\",\n    \"Comprehensive testing strategy\",\n    \"CI/CD pipeline with security scanning\",\n    \"Container orchestration with monitoring\",\n    \"Model retraining automation\",\n    \"Production-ready deployment\"\n]\n\nfor i, feature in enumerate(features, 1):\n    print(f\"   {i}. {feature}\")\n\nprint(f\"\\nðŸŽ‰ Project completed successfully! All MLOps requirements implemented.\")",
   "metadata": {
    "id": "project_summary"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}