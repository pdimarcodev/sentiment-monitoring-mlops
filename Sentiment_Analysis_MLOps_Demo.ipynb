{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# ğŸ¢ MLOps Sentiment Analysis - Company Reputation Monitoring\n\n**Project**: Online Reputation Monitoring System for MachineInnovators Inc.  \n**GitHub Repository**: https://github.com/pdimarcodev/sentiment-monitoring-mlops\n\nThis notebook demonstrates the complete MLOps pipeline for sentiment analysis including:\n- âœ… HuggingFace RoBERTa model integration\n- âœ… FastAPI service with comprehensive endpoints\n- âœ… Automated testing and CI/CD pipeline\n- âœ… Grafana monitoring and metrics\n- âœ… Docker containerization\n- âœ… Automated model retraining with Airflow",
   "metadata": {
    "id": "title"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸš€ Setup and Installation"
   ],
   "metadata": {
    "id": "setup"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch fastapi uvicorn pydantic requests pandas numpy scikit-learn\n",
    "!pip install pytest pytest-asyncio httpx\n",
    "!pip install prometheus-client datasets"
   ],
   "metadata": {
    "id": "install"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ“¥ Clone Repository and Setup"
   ],
   "metadata": {
    "id": "clone"
   }
  },
  {
   "cell_type": "code",
   "source": "# Clone the repository\n!git clone https://github.com/pdimarcodev/sentiment-monitoring-mlops.git\n%cd sentiment-monitoring-mlops\n\n# List project structure\n!find . -type f -name \"*.py\" | head -20",
   "metadata": {
    "id": "clone_repo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ¤– Load and Test Sentiment Analysis Model"
   ],
   "metadata": {
    "id": "model_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Import our custom sentiment analyzer\n",
    "import sys\n",
    "sys.path.append('/content/sentiment-monitoring-mlops')\n",
    "\n",
    "from src.sentiment_analyzer.model import SentimentAnalyzer\n",
    "import json\n",
    "\n",
    "# Initialize the model\n",
    "print(\"Loading sentiment analysis model...\")\n",
    "analyzer = SentimentAnalyzer()\n",
    "print(\"âœ… Model loaded successfully!\")\n",
    "\n",
    "# Display model info\n",
    "model_info = analyzer.get_model_info()\n",
    "print(f\"\\nğŸ“‹ Model Information:\")\n",
    "for key, value in model_info.items():\n",
    "    print(f\"  {key}: {value}\")"
   ],
   "metadata": {
    "id": "load_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ§ª Test Single Predictions"
   ],
   "metadata": {
    "id": "single_test"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Test with various sentiment examples\n",
    "test_texts = [\n",
    "    \"I absolutely love this company's products! Best service ever!\",\n",
    "    \"This service is terrible and I'm very disappointed.\",\n",
    "    \"The product is okay, nothing particularly special.\",\n",
    "    \"Amazing customer support and fast delivery! Highly recommend!\",\n",
    "    \"Poor quality for the price. Won't buy again.\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª Testing Single Predictions:\\n\")\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    result = analyzer.predict(text)\n",
    "    \n",
    "    # Add emoji based on sentiment\n",
    "    emoji = {\"positive\": \"ğŸ˜Š\", \"negative\": \"ğŸ˜\", \"neutral\": \"ğŸ˜\"}\n",
    "    sentiment_emoji = emoji.get(result['sentiment'], '')\n",
    "    \n",
    "    print(f\"{i}. Text: \\\"{text}\\\"\")\n",
    "    print(f\"   {sentiment_emoji} Sentiment: {result['sentiment'].upper()} ({result['confidence']:.2%})\")\n",
    "    print(f\"   All scores: {result['all_scores']}\")\n",
    "    print()"
   ],
   "metadata": {
    "id": "single_predictions"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ“Š Test Batch Predictions"
   ],
   "metadata": {
    "id": "batch_test"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Test batch predictions\n",
    "batch_texts = [\n",
    "    \"Great product, fast shipping!\",\n",
    "    \"Customer service was unhelpful.\",\n",
    "    \"Average quality for the price.\",\n",
    "    \"Exceeded my expectations!\",\n",
    "    \"Worst purchase I've made.\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“Š Testing Batch Predictions:\\n\")\n",
    "batch_results = analyzer.predict_batch(batch_texts)\n",
    "\n",
    "# Display results\n",
    "sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
    "\n",
    "for i, result in enumerate(batch_results, 1):\n",
    "    emoji = {\"positive\": \"ğŸ˜Š\", \"negative\": \"ğŸ˜\", \"neutral\": \"ğŸ˜\"}\n",
    "    sentiment_emoji = emoji.get(result['sentiment'], '')\n",
    "    \n",
    "    print(f\"{i}. {sentiment_emoji} {result['sentiment'].upper()} ({result['confidence']:.2%}): \\\"{result['text']}\\\"\")\n",
    "    sentiment_counts[result['sentiment']] += 1\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Summary:\")\n",
    "total = len(batch_results)\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    percentage = count / total * 100\n",
    "    emoji = {\"positive\": \"ğŸ˜Š\", \"negative\": \"ğŸ˜\", \"neutral\": \"ğŸ˜\"}\n",
    "    print(f\"  {emoji[sentiment]} {sentiment.title()}: {count}/{total} ({percentage:.1f}%)\")"
   ],
   "metadata": {
    "id": "batch_predictions"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸš€ Start FastAPI Service"
   ],
   "metadata": {
    "id": "fastapi_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Start the FastAPI service in the background\n",
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "\n",
    "def start_api_server():\n",
    "    \"\"\"Start the FastAPI server in a separate thread\"\"\"\n",
    "    subprocess.run([\"python\", \"main.py\"], cwd=\"/content/sentiment-monitoring-mlops\")\n",
    "\n",
    "# Start the server in background\n",
    "api_thread = threading.Thread(target=start_api_server, daemon=True)\n",
    "api_thread.start()\n",
    "\n",
    "print(\"ğŸš€ Starting FastAPI server...\")\n",
    "time.sleep(10)  # Wait for server to start\n",
    "print(\"âœ… API server should be running on http://localhost:8000\")"
   ],
   "metadata": {
    "id": "start_api"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ§ª Test API Endpoints"
   ],
   "metadata": {
    "id": "api_test_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "\n",
    "# Test health endpoint\n",
    "print(\"ğŸ¥ Testing Health Endpoint:\")\n",
    "try:\n",
    "    response = requests.get(f\"{BASE_URL}/health\", timeout=5)\n",
    "    print(f\"Status: {response.status_code}\")\n",
    "    print(f\"Response: {json.dumps(response.json(), indent=2)}\")\nexcept Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test single prediction endpoint\n",
    "print(\"ğŸ¤– Testing Single Prediction Endpoint:\")\n",
    "try:\n",
    "    payload = {\"text\": \"I love this company's innovative solutions!\"}\n",
    "    response = requests.post(f\"{BASE_URL}/predict\", json=payload, timeout=10)\n",
    "    print(f\"Status: {response.status_code}\")\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        emoji = {\"positive\": \"ğŸ˜Š\", \"negative\": \"ğŸ˜\", \"neutral\": \"ğŸ˜\"}\n",
    "        sentiment_emoji = emoji.get(result['sentiment'], '')\n",
    "        print(f\"Text: \\\"{result['text']}\\\"\")\n",
    "        print(f\"Sentiment: {sentiment_emoji} {result['sentiment'].upper()} ({result['confidence']:.2%})\")\n",
    "        print(f\"All scores: {result['all_scores']}\")\n",
    "    else:\n",
    "        print(f\"Error: {response.text}\")\nexcept Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ],
   "metadata": {
    "id": "test_api"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ“Š Test Batch API Endpoint"
   ],
   "metadata": {
    "id": "batch_api_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Test batch prediction endpoint\n",
    "print(\"ğŸ“Š Testing Batch Prediction Endpoint:\")\n",
    "try:\n",
    "    batch_payload = {\n",
    "        \"texts\": [\n",
    "            \"Excellent product quality and service!\",\n",
    "            \"Poor customer experience, very disappointed.\",\n",
    "            \"The product is decent, meets basic needs.\",\n",
    "            \"Outstanding innovation and user experience!\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(f\"{BASE_URL}/predict/batch\", json=batch_payload, timeout=15)\n",
    "    print(f\"Status: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"Total processed: {result['total_processed']}\\n\")\n",
    "        \n",
    "        # Display results\n",
    "        sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
    "        \n",
    "        for i, pred in enumerate(result['results'], 1):\n",
    "            emoji = {\"positive\": \"ğŸ˜Š\", \"negative\": \"ğŸ˜\", \"neutral\": \"ğŸ˜\"}\n",
    "            sentiment_emoji = emoji.get(pred['sentiment'], '')\n",
    "            print(f\"{i}. {sentiment_emoji} {pred['sentiment'].upper()} ({pred['confidence']:.2%})\")\n",
    "            print(f\"   Text: \\\"{pred['text']}\\\"\")\n",
    "            sentiment_counts[pred['sentiment']] += 1\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ Batch Summary:\")\n",
    "        for sentiment, count in sentiment_counts.items():\n",
    "            percentage = count / result['total_processed'] * 100\n",
    "            emoji = {\"positive\": \"ğŸ˜Š\", \"negative\": \"ğŸ˜\", \"neutral\": \"ğŸ˜\"}\n",
    "            print(f\"  {emoji[sentiment]} {sentiment.title()}: {count} ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"Error: {response.text}\")\nexcept Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ],
   "metadata": {
    "id": "batch_api_test"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ“ˆ Test Metrics Endpoint"
   ],
   "metadata": {
    "id": "metrics_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Test metrics endpoint\n",
    "print(\"ğŸ“ˆ Testing Metrics Endpoint:\")\n",
    "try:\n",
    "    response = requests.get(f\"{BASE_URL}/metrics\", timeout=5)\n",
    "    print(f\"Status: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        metrics_data = response.text\n",
    "        print(\"\\nğŸ“Š Prometheus Metrics Sample:\")\n",
    "        # Show only sentiment-related metrics\n",
    "        for line in metrics_data.split('\\n')[:20]:\n",
    "            if 'sentiment' in line.lower() and not line.startswith('#'):\n",
    "                print(f\"  {line}\")\n",
    "    else:\n",
    "        print(f\"Error: {response.text}\")\nexcept Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ],
   "metadata": {
    "id": "test_metrics"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ§ª Run Automated Tests"
   ],
   "metadata": {
    "id": "testing_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Run the automated test suite\n",
    "print(\"ğŸ§ª Running Automated Test Suite:\\n\")\n",
    "\n",
    "# Run model tests\n",
    "print(\"Testing model functionality...\")\n",
    "!cd /content/sentiment-monitoring-mlops && python -m pytest tests/test_model.py -v\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Run API tests\n",
    "print(\"Testing API functionality...\")\n",
    "!cd /content/sentiment-monitoring-mlops && python -m pytest tests/test_api.py -v"
   ],
   "metadata": {
    "id": "run_tests"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ³ Docker Build Test"
   ],
   "metadata": {
    "id": "docker_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Test Docker build (if Docker is available)\n",
    "print(\"ğŸ³ Testing Docker Build:\\n\")\n",
    "\n",
    "try:\n",
    "    # Check if Docker is available\n",
    "    !docker --version\n",
    "    \n",
    "    # Build the Docker image\n",
    "    print(\"\\nBuilding Docker image...\")\n",
    "    !cd /content/sentiment-monitoring-mlops && docker build -t sentiment-analyzer:colab-test .\n",
    "    \n",
    "    print(\"\\nâœ… Docker build completed successfully!\")\n",
    "    \n",
    "    # Show image info\n",
    "    !docker images sentiment-analyzer:colab-test\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"âš ï¸ Docker not available in this environment: {e}\")\n",
    "    print(\"Docker build would work in a local environment with Docker installed.\")"
   ],
   "metadata": {
    "id": "docker_test"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ“‹ MLOps Pipeline Summary"
   ],
   "metadata": {
    "id": "summary_section"
   }
  },
  {
   "cell_type": "code",
   "source": "# Display comprehensive project summary\nprint(\"ğŸ¯ MLOps SENTIMENT ANALYSIS PROJECT SUMMARY\\n\")\nprint(\"=\"*60)\n\nsummary = {\n    \"âœ… Model Implementation\": \"HuggingFace RoBERTa (cardiffnlp/twitter-roberta-base-sentiment-latest)\",\n    \"âœ… API Service\": \"FastAPI with /predict, /predict/batch, /health, /metrics endpoints\",\n    \"âœ… Testing Suite\": \"Comprehensive unit and integration tests with pytest\",\n    \"âœ… CI/CD Pipeline\": \"GitHub Actions with automated testing and deployment\",\n    \"âœ… Containerization\": \"Multi-stage Docker build with security best practices\",\n    \"âœ… Monitoring\": \"Grafana + Prometheus with custom dashboards\",\n    \"âœ… Orchestration\": \"Docker Compose for local development\",\n    \"âœ… Model Retraining\": \"Airflow DAG for automated model updates\",\n    \"âœ… Deployment Ready\": \"HuggingFace Spaces integration with Gradio UI\"\n}\n\nfor feature, description in summary.items():\n    print(f\"{feature}\")\n    print(f\"   {description}\\n\")\n\nprint(\"ğŸ”— Key URLs:\")\nprint(\"   â€¢ GitHub Repository: https://github.com/pdimarcodev/sentiment-monitoring-mlops\")\nprint(\"   â€¢ Grafana Dashboard: http://localhost:3000 (when running locally)\")\n\nprint(\"\\nğŸš€ Production Deployment Steps:\")\nprint(\"   1. Push code to GitHub repository\")\nprint(\"   2. Configure GitHub Secrets (DOCKER_USERNAME, DOCKER_PASSWORD, HF_TOKEN)\")\nprint(\"   3. Deploy with: docker-compose up -d\")\nprint(\"   4. Access Grafana at http://localhost:3000 (admin/admin123)\")\nprint(\"   5. Monitor metrics and sentiment trends\")\n\nprint(\"\\nâœ¨ MLOps Features Demonstrated:\")\nfeatures = [\n    \"Automated model loading and inference\",\n    \"RESTful API with proper error handling\",\n    \"Prometheus metrics collection\",\n    \"Comprehensive testing strategy\",\n    \"CI/CD pipeline with security scanning\",\n    \"Container orchestration with monitoring\",\n    \"Model retraining automation\",\n    \"Production-ready deployment\"\n]\n\nfor i, feature in enumerate(features, 1):\n    print(f\"   {i}. {feature}\")\n\nprint(f\"\\nğŸ‰ Project completed successfully! All MLOps requirements implemented.\")",
   "metadata": {
    "id": "project_summary"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}